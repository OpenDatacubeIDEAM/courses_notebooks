{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "print(\"Numpy version: \"+np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de conjunto XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.5 # ruido\n",
    "N = 500\n",
    "\n",
    "X = [1.0, -1.0] + s*np.random.randn(N, 2)\n",
    "tmp = [-1.0, 1.0] + s*np.random.randn(N, 2)\n",
    "X = np.concatenate((X, tmp))\n",
    "tmp = [1.0, 1.0] + s*np.random.randn(N, 2)\n",
    "X = np.concatenate((X, tmp))\n",
    "tmp = [-1.0, -1.0] + s*np.random.randn(N, 2)\n",
    "X = np.concatenate((X, tmp))\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "labels = np.zeros(shape=(4*N,), dtype=int)\n",
    "labels[2*N:] = 1.0\n",
    "\n",
    "# Conjuntos de entrenamiento y prueba\n",
    "train_p = 0.25\n",
    "P = np.random.permutation(2*N)\n",
    "index_train = np.concatenate((np.arange(0, 4*N)[labels==0][P[:int(train_p*2*N)]], \n",
    "                              np.arange(0, 4*N)[labels==1][P[:int(train_p*2*N)]]))\n",
    "index_test = np.concatenate((np.arange(0, 4*N)[labels==0][P[int(train_p*2*N):]], \n",
    "                             np.arange(0, 4*N)[labels==1][P[int(train_p*2*N):]]))\n",
    "\n",
    "print(\"%d samples for train and %d for testing\" %(len(index_train), len(index_test)))\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[labels==0, 0], X[labels==0, 1], color='b', marker='o', label=\"Class 1\", alpha=0.5)\n",
    "ax.scatter(X[labels==1, 0], X[labels==1, 1], color='r', marker='o', label=\"Class 2\", alpha=0.5)\n",
    "ax.grid()\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación MLP con numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, N_input, N_neurons, activation=\"tanh\"):        \n",
    "        self.N_input = N_input\n",
    "        self.N_neurons = N_neurons\n",
    "        self.activation = activation\n",
    "        self.W = np.zeros(shape=(N_input, N_neurons))\n",
    "        self.b = np.zeros(shape=(N_neurons, ))\n",
    "        self.initialize_parameters()  \n",
    "        self.z = np.zeros(shape=(N_neurons))\n",
    "        self.y = np.zeros(shape=(N_neurons))\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        if self.activation == \"tanh\":\n",
    "            self.W = np.array(np.random.uniform(low=-np.sqrt(6.0/(self.N_input + self.N_neurons)),\n",
    "                                                high=np.sqrt(6.0/(self.N_input + self.N_neurons)),\n",
    "                                                size=(self.N_input, self.N_neurons)))\n",
    "        else: \n",
    "            self.W = np.array(np.zeros(shape=(self.N_input, self.N_neurons)))\n",
    "        self.b = np.zeros(shape=(self.N_neurons, ))\n",
    "        \n",
    "    def output(self, x):  #  x is an array N_input x 1\n",
    "        self.z = np.dot(x, self.W) + self.b\n",
    "        if self.activation == \"tanh\":\n",
    "            self.y = np.tanh(self.z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            self.y = sigmoid(self.z)\n",
    "        return self.y\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, N_input, N_hidden, N_output):\n",
    "        self.layers = []\n",
    "        self.N_output = N_output\n",
    "        self.output_activation = \"sigmoid\"\n",
    "        if N_hidden > 0:\n",
    "            self.layers.append(Layer(N_input, N_hidden))\n",
    "            self.layers.append(Layer(N_hidden, N_output, activation=\"sigmoid\"))\n",
    "        else:\n",
    "            self.layers.append(Layer(N_input, N_output, activation=\"sigmoid\"))\n",
    "            \n",
    "    def predict_proba(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.output(x)\n",
    "        return x\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x = self.predict_proba(x)\n",
    "        if self.N_output > 1:\n",
    "            return np.argmax(x, axis=1)\n",
    "        else:\n",
    "            if self.output_activation is \"sigmoid\":\n",
    "                return x > 0.5\n",
    "            elif self.output_activation is \"tanh\":\n",
    "                return x > 0\n",
    "        \n",
    "    def reset(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_parameters()\n",
    "    \n",
    "    def mse(self, x, y):\n",
    "        e = self.predict_proba(x)[:,0] - y\n",
    "        return np.mean(np.power(e, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, classifier, learning_rate=1.0, max_epoch=1000):\n",
    "        self.classifier = classifier\n",
    "        self.lr = learning_rate\n",
    "        self.train_cost = np.zeros(shape=(max_epoch,))\n",
    "        self.test_cost = np.zeros(shape=(max_epoch, ))   \n",
    "        self.max_epoch = max_epoch\n",
    "        self.epoch = 0\n",
    "\n",
    "    def update(self, data_train, label_train, data_test, label_test, batch_size=32):\n",
    "        if self.epoch == 0:  \n",
    "            self.train_cost[0] =  self.classifier.mse(data_train, label_train) \n",
    "            self.test_cost[0] =  self.classifier.mse(data_test, label_test) \n",
    "        # Present mini-batches in different order\n",
    "        else:        \n",
    "            rand_perm = np.random.permutation(len(label_train))\n",
    "            data_train = data_train[rand_perm, :].copy()\n",
    "            label_train = label_train[rand_perm].copy()\n",
    "            self.minibach_eval(data_train, label_train, batch_size)\n",
    "            self.train_cost[self.epoch] = self.classifier.mse(data_train, label_train) \n",
    "            self.test_cost[self.epoch] =  self.classifier.mse(data_test, label_test)               \n",
    "        self.epoch += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.classifier.reset()        \n",
    "\n",
    "    def minibach_eval(self, data, labels, batch_size=32):\n",
    "        averaged_cost = 0.0\n",
    "        N = len(labels)\n",
    "        for Nbatches, (start, end) in enumerate(zip(range(0, N, batch_size), range(batch_size, N+1, batch_size))):\n",
    "            self.train(data[start:end], labels[start:end])\n",
    "        #return averaged_cost/Nbatches\n",
    "    \n",
    "    def train(self, data, labels):\n",
    "        # do training\n",
    "        N = len(labels)\n",
    "        error = (self.classifier.predict_proba(data)[:,0] - labels)[:, np.newaxis]\n",
    "        Nlayer = len(self.classifier.layers)\n",
    "        # outer layer with one neuron\n",
    "        layer_idx = Nlayer-1 \n",
    "        out_layer = self.classifier.layers[layer_idx]\n",
    "        tmp = np.multiply(error, sigmoid(out_layer.z)*(1.0-sigmoid(out_layer.z)))\n",
    "        grad_b = np.mean(tmp, axis=0)\n",
    "        if Nlayer == 1:\n",
    "            grad_w = np.mean(np.multiply(np.tile(tmp, (1, out_layer.N_input)), data), axis=0)\n",
    "        else:\n",
    "            hidden_layer = self.classifier.layers[layer_idx-1]\n",
    "            #print(hidden_layer.y.shape)\n",
    "            #print(tmp.shape)\n",
    "            grad_w = np.mean(np.multiply(np.tile(tmp, (1, out_layer.N_input)), hidden_layer.y), axis=0) \n",
    "            #print(grad_w.shape)\n",
    "        self.classifier.layers[layer_idx].W -= self.lr*grad_w[:, np.newaxis]\n",
    "        self.classifier.layers[layer_idx].b -= self.lr*grad_b\n",
    "         # hidden layer\n",
    "        if Nlayer > 1:\n",
    "            Nh = out_layer.N_input\n",
    "            E = np.tile(error*sigmoid(out_layer.z)*(1.0-sigmoid(out_layer.z)), (1, Nh))\n",
    "            tmp = np.multiply(np.multiply(E, 1.0-np.tanh(hidden_layer.z)**2), np.tile(out_layer.W.T, (N, 1)))\n",
    "            grad_b = np.mean(tmp, axis=0)\n",
    "            grad_w = np.dot(tmp.T, data).T\n",
    "            #print(grad_w)\n",
    "            self.classifier.layers[layer_idx-1].W -= self.lr*grad_w\n",
    "            self.classifier.layers[layer_idx-1].b -= self.lr*grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = MLP(N_input=X.shape[1], N_hidden=5, N_output=1)\n",
    "trainer = Trainer(classifier, learning_rate=0.5, max_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de la red en vivo \n",
    "\n",
    "Ejecuta el siguiente bloque varias veces para agregar épocas (CTRL+ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corre CTRL+ENTER\n",
    "trainer.update(X[index_train, :], labels[index_train], \n",
    "              X[index_test, :], labels[index_test], batch_size=10)\n",
    "\n",
    "pred_labels = classifier.predict(X)\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.jet, alpha=0.5)\n",
    "ax.scatter(X[labels==0, 0], X[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(X[labels==1, 0], X[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(np.arange(0, trainer.epoch, step=1), trainer.train_cost[:trainer.epoch], 'b-o', label=\"train MSE\")\n",
    "ax.plot(np.arange(0, trainer.epoch, step=1), trainer.test_cost[:trainer.epoch], 'g-o', label=\"test MSE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación MLP con tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \"+tf.__version__)\n",
    "\n",
    "Nh = 10 # Neuronas en la capa oculta\n",
    "X_holder = tf.placeholder(tf.float32, [None, 2])\n",
    "T_holder = tf.placeholder(tf.float32, [None, 1])\n",
    "# Armando la red\n",
    "b_z = tf.Variable(tf.zeros([Nh]), name=\"z_b\", dtype=tf.float32)\n",
    "W_z = tf.Variable(tf.random_uniform([2, Nh], -1.0, 1.0), name=\"z_w\", dtype=tf.float32)\n",
    "z = tf.tanh(tf.matmul(X_holder, W_z) + b_z)\n",
    "b_y = tf.Variable(tf.zeros([1]), name=\"y_b\", dtype=tf.float32)\n",
    "W_y = tf.Variable(tf.random_uniform([Nh, 1], -1.0, 1.0), name=\"y_w\", dtype=tf.float32)\n",
    "y = tf.add(tf.matmul(z, W_y), b_y)\n",
    "# Función de costo y optimizador\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=T_holder, logits=y)\n",
    "loss = tf.reduce_mean(cross_entropy)  \n",
    "optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "train = optimizer.minimize(loss) \n",
    "# Inicializando las variables y creando una sesión te tensorflow\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)  \n",
    "\n",
    "nepochs = 500\n",
    "train_loss = np.zeros(shape=(nepochs))\n",
    "test_loss = np.zeros(shape=(nepochs))\n",
    "for i, epoch in enumerate(range(0, nepochs)):\n",
    "    # Entrenamos una epoca\n",
    "    _, train_loss[i] = sess.run([train, loss], feed_dict={X_holder: X[index_train, :], \n",
    "                                                     T_holder: np.reshape(labels[index_train], [-1, 1])})\n",
    "    # Evaluamos en el conjunto de test\n",
    "    pred_test, test_loss[i] = sess.run([y, loss], feed_dict={X_holder: X[index_test, :], \n",
    "                                                             T_holder: np.reshape(labels[index_test], [-1, 1])})\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(test_loss, label='Test')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(X[index_test, :][labels[index_test]==0, 0], \n",
    "           X[index_test, :][labels[index_test]==0, 1], \n",
    "           c=pred_test[labels[index_test]==0], marker='o', \n",
    "           alpha=0.5, vmin=0.0, vmax=1.0, cmap=plt.cm.RdBu_r)\n",
    "sc = ax.scatter(X[index_test, :][labels[index_test]==1, 0], \n",
    "                X[index_test, :][labels[index_test]==1, 1], \n",
    "                c=pred_test[labels[index_test]==1], marker='x', \n",
    "                alpha=0.5, vmin=0.0, vmax=1.0, cmap=plt.cm.RdBu_r)\n",
    "plt.colorbar(sc)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación MLP Bayesiana con pymc y theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://docs.pymc.io/notebooks/bayesian_neural_network_advi.html\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import theano.tensor as tt\n",
    "print(\"Theano version: \"+theano.__version__)\n",
    "print(\"Pymc3 version: \"+pm.__version__)\n",
    "\n",
    "ann_input = theano.shared(X[index_train, :])\n",
    "ann_output = theano.shared(labels[index_train])\n",
    "\n",
    "with pm.Model() as neural_network:\n",
    "    # Create network (priors + posteriors)\n",
    "    W_z = pm.Normal('wz', 0, sd=1, shape=(X.shape[1], Nh),\n",
    "                    testval=np.random.randn(X.shape[1], Nh).astype(floatX))\n",
    "    b_z = pm.Normal('bz', 0, sd=1, shape=(Nh, ), testval=np.random.randn(Nh).astype(floatX))\n",
    "    W_y = pm.Normal('wy', 0, sd=1, shape=(Nh,), \n",
    "                    testval=np.random.randn(Nh).astype(floatX))\n",
    "    b_y = pm.Normal('by', 0, sd=1, shape=(1,), testval=np.random.randn(1).astype(floatX))\n",
    "\n",
    "    z = pm.math.tanh(pm.math.dot(ann_input, W_z) + b_z)\n",
    "    y = pm.math.sigmoid(pm.math.dot(z, W_y) + b_y)\n",
    "    y_rv = pm.Bernoulli('y', y, observed=ann_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with neural_network:\n",
    "    # Run ADVI to train\n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=30000, method=inference)\n",
    "    # Create a theano function to evaluate new samples\n",
    "    x = tt.matrix('X')\n",
    "    n = tt.iscalar('n')\n",
    "    x.tag.test_value = np.empty_like(X[index_test, :])\n",
    "    n.tag.test_value = 100\n",
    "    _sample_proba = approx.sample_node(y_rv.distribution.p, size=n,\n",
    "                                       more_replacements={ann_input: x})\n",
    "\n",
    "    sample_proba = theano.function([x, n], _sample_proba)\n",
    "    \n",
    "# Predicción en el espacio de entrada\n",
    "pred = sample_proba(X[index_test, :], 500).mean(0) > 0.5\n",
    "grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\n",
    "grid_2d = grid.reshape(2, -1).T\n",
    "dummy_out = np.ones(grid.shape[1], dtype=np.int8)\n",
    "ppc = sample_proba(grid_2d, 500)\n",
    "# Evolución del ELBO\n",
    "plt.plot(inference.hist, alpha=.5)\n",
    "plt.ylabel('ELBO')\n",
    "plt.xlabel('iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones (media)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "contour = ax.contourf(grid[0], grid[1], ppc.mean(axis=0).reshape(100, 100), \n",
    "                      vmin=0.0, vmax=1.0, cmap=plt.cm.RdBu_r, alpha=0.5)\n",
    "ax.scatter(X[index_test, :][pred==0, 0], X[index_test, :][pred==0, 1], marker='o', color='k')\n",
    "ax.scatter(X[index_test, :][pred==1, 0], X[index_test, :][pred==1, 1], marker='x', color='k')\n",
    "cbar = plt.colorbar(contour, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones (desviación estándar)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "contour = ax.contourf(grid[0], grid[1], ppc.std(axis=0).reshape(100, 100), \n",
    "                      cmap=plt.cm.Purples, alpha=0.5)\n",
    "ax.scatter(X[index_test, :][pred==0, 0], X[index_test, :][pred==0, 1], marker='o', color='k')\n",
    "ax.scatter(X[index_test, :][pred==1, 0], X[index_test, :][pred==1, 1], marker='x', color='k')\n",
    "cbar = plt.colorbar(contour, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
