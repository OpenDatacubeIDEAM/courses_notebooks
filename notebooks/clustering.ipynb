{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Numpy version: \"+np.__version__)\n",
    "from sklearn import __version__ as skver\n",
    "print(\"Scikit-learn version: \"+skver)\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift \n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "print(\"Matplotlib version: \"+plt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # number of samples\n",
    "X, Y = make_blobs(n_samples=N, centers=[[1, 1], [-1, -1]], cluster_std=0.5)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', \n",
    "           s=50, alpha=0.75, label='class 1')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', \n",
    "           s=50, alpha=0.75, label='class 2')\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla k-means from sklearn\n",
    "\n",
    "- The user has to set the number of clusters. Try n_clusters $\\in \\{2, 3, 5 \\}$\n",
    "- The number of random initializations is set with n_init\n",
    "- There is also a mini-batch implementation for large databases: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = KMeans(n_clusters=2, init='random', n_init=10)\n",
    "pred = cluster_method.fit_predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c=pred[Y==0], marker='x', \n",
    "           s=50, alpha=0.75, label='class 1', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.n_clusters+1)\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c=pred[Y==1], marker='o', \n",
    "           s=50, alpha=0.75, label='class 2', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.n_clusters+1)\n",
    "\n",
    "for point in cluster_method.cluster_centers_:\n",
    "    ax.scatter(point[0], point[1], c='k', marker='d', s=200, alpha=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom k-means and k-medians\n",
    "\n",
    "- Euclidean distance is replaced by Manhattan, and mean is replaced by the median\n",
    "- Implementation with minor modifications from https://gist.github.com/mblondel/1451300\n",
    "- The user has to set the number of clusters. Try $k \\in \\{2, 3, 5\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans_(BaseEstimator):\n",
    "\n",
    "    def __init__(self, k, max_iter=100, random_state=0, tol=1e-4):\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.tol = tol\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        self.labels_ = euclidean_distances(X, self.cluster_centers_,\n",
    "                                     squared=True).argmin(axis=1)\n",
    "\n",
    "    def _average(self, X):\n",
    "        return X.mean(axis=0)\n",
    "\n",
    "    def _m_step(self, X):\n",
    "        X_center = None\n",
    "        for center_id in range(self.k):\n",
    "            center_mask = self.labels_ == center_id\n",
    "            if not np.any(center_mask):\n",
    "                # The centroid of empty clusters is set to the center of\n",
    "                # everything\n",
    "                if X_center is None:\n",
    "                    X_center = self._average(X)\n",
    "                self.cluster_centers_[center_id] = X_center\n",
    "            else:\n",
    "                self.cluster_centers_[center_id] = \\\n",
    "                    self._average(X[center_mask])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n_samples = X.shape[0]\n",
    "        vdata = np.mean(np.var(X, 0))\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        self.labels_ = random_state.permutation(n_samples)[:self.k]\n",
    "        self.cluster_centers_ = X[self.labels_]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            centers_old = self.cluster_centers_.copy()\n",
    "\n",
    "            self._e_step(X)\n",
    "            self._m_step(X)\n",
    "\n",
    "            if np.sum((centers_old - self.cluster_centers_) ** 2) < self.tol * vdata:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "    \n",
    "class KMedians(KMeans_):\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        self.labels_ = manhattan_distances(X, self.cluster_centers_).argmin(axis=1)\n",
    "\n",
    "    def _average(self, X):\n",
    "        return np.median(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = KMeans_(k=2, max_iter=0).fit(X)\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "for i in range(5):\n",
    "    cluster_method._e_step(X)\n",
    "    pred = cluster_method.labels_\n",
    "    \n",
    "    ax = fig.add_subplot(1, 5, i+1)\n",
    "    ax.scatter(X[Y==0, 0], X[Y==0, 1], c=pred[Y==0], marker='x', \n",
    "               s=50, alpha=0.75, label='class 1', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.k+1)\n",
    "    ax.scatter(X[Y==1, 0], X[Y==1, 1], c=pred[Y==1], marker='o', \n",
    "               s=50, alpha=0.75, label='class 2', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.k+1)\n",
    "    for point in cluster_method.cluster_centers_:\n",
    "        ax.scatter(point[0], point[1], c='k', marker='d', s=200, alpha=.5)\n",
    "    plt.title('Iteration %d' %(i))\n",
    "    cluster_method._m_step(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "noisy_points = [-1, -1] + 6*np.random.randn(100, 2)\n",
    "X_outliers = np.concatenate((X, noisy_points), axis=0)\n",
    "Y_outliers = np.concatenate((Y, np.ones(shape=(100,))), axis=0)\n",
    "\n",
    "cluster_method = KMeans_(k=2).fit(X_outliers)\n",
    "pred = cluster_method.labels_\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.scatter(X_outliers[Y_outliers==0, 0], X_outliers[Y_outliers==0, 1], c=pred[Y_outliers==0], marker='x', \n",
    "           s=50, alpha=0.75, label='class 1', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.k+1)\n",
    "ax.scatter(X_outliers[Y_outliers==1, 0], X_outliers[Y_outliers==1, 1], c=pred[Y_outliers==1], marker='o', \n",
    "           s=50, alpha=0.75, label='class 2', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.k+1)\n",
    "for point in cluster_method.cluster_centers_:\n",
    "    ax.scatter(point[0], point[1], c='k', marker='d', s=200, alpha=.5)\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylim([-5, 5])\n",
    "plt.title('K-means')\n",
    "\n",
    "cluster_method = KMedians(k=2).fit(X_outliers)\n",
    "pred = cluster_method.labels_\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.scatter(X_outliers[Y_outliers==0, 0], X_outliers[Y_outliers==0, 1], c=pred[Y_outliers==0], marker='x', \n",
    "           s=50, alpha=0.75, label='class 1', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.k+1)\n",
    "ax.scatter(X_outliers[Y_outliers==1, 0], X_outliers[Y_outliers==1, 1], c=pred[Y_outliers==1], marker='o', \n",
    "           s=50, alpha=0.75, label='class 2', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.k+1)\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylim([-5, 5])\n",
    "for point in cluster_method.cluster_centers_:\n",
    "    ax.scatter(point[0], point[1], c='k', marker='d', s=200, alpha=.5)\n",
    "plt.title('K-medians')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy c-means\n",
    "- Gives cluster probabilities instead of cluster labels \n",
    "- Implementation with minor modifications from https://gist.github.com/mblondel/1451300\n",
    "- Parameter $q > 1$ controls the \"fuzzyness\" and $k$ the number of clusters\n",
    "- Bezdek, James C. (1981). Pattern Recognition with Fuzzy Objective Function Algorithms. ISBN 0-306-40671-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FuzzyCMeans(KMeans_):\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        D = 1.0 / euclidean_distances(X, self.cluster_centers_, squared=True)\n",
    "        D **= 1.0 / (self.q - 1)\n",
    "        D /= np.sum(D, axis=1)[:, np.newaxis]\n",
    "        # shape: n_samples x k\n",
    "        self.fuzzy_labels_ = D\n",
    "        self.labels_ = self.fuzzy_labels_.argmax(axis=1)\n",
    "        \n",
    "    def _m_step(self, X):\n",
    "        weights = self.fuzzy_labels_ ** self.q\n",
    "        # shape: n_clusters x n_features\n",
    "        self.cluster_centers_ = np.dot(X.T, weights).T\n",
    "        self.cluster_centers_ /= weights.sum(axis=0)[:, np.newaxis]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        D = 1.0 / euclidean_distances(X, self.cluster_centers_, squared=True)\n",
    "        D **= 1.0 / (self.q - 1)\n",
    "        D /= np.sum(D, axis=1)[:, np.newaxis]\n",
    "        return D\n",
    "    \n",
    "    def fit(self, X, y=None, q=2):\n",
    "        n_samples, n_features = X.shape\n",
    "        vdata = np.mean(np.var(X, 0))\n",
    "        self.q = q\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        self.fuzzy_labels_ = random_state.rand(n_samples, self.k)\n",
    "        self.fuzzy_labels_ /= self.fuzzy_labels_.sum(axis=1)[:, np.newaxis]\n",
    "        self._m_step(X)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            centers_old = self.cluster_centers_.copy()\n",
    "\n",
    "            self._e_step(X)\n",
    "            self._m_step(X)\n",
    "\n",
    "            if np.sum((centers_old - self.cluster_centers_) ** 2) < self.tol * vdata:\n",
    "                break\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = FuzzyCMeans(k=2)\n",
    "cluster_method.fit(X)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', \n",
    "           s=50, alpha=0.75, label='class 1')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', \n",
    "           s=50, alpha=0.75, label='class 2')\n",
    "Z = cluster_method.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "cr = ax.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5, vmin=0, vmax=1)\n",
    "plt.colorbar(cr)\n",
    "for point in cluster_method.cluster_centers_:\n",
    "    ax.scatter(point[0], point[1], c='k', marker='d', s=200, alpha=.5)\n",
    "plt.title('Fuzzy c-means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN (Density-based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "- Uses the notion of sample density to find clusters\n",
    "- Finds the number of clusters automatically\n",
    "- It has two parameters: \n",
    "    - min_samples is the minimum number of neighbors a sample needs to become a core point\n",
    "    - eps is the radius in which sample density is measured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = DBSCAN(min_samples=10, eps=0.25)\n",
    "pred = cluster_method.fit_predict(X)\n",
    "n_clusters = len(np.unique(pred)) \n",
    "print(\"Number of clusters: %d\" %(n_clusters))\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for label, c in zip(np.unique(pred), plt.cm.Accent(np.linspace(0, 1, n_clusters))):\n",
    "    if label == -1: # noise cluster\n",
    "        ax.scatter(X[pred==label, 0], X[pred==label,1], color=c, marker='x', \n",
    "                   s=50, alpha=0.75)\n",
    "    else:\n",
    "        ax.scatter(X[pred==label, 0], X[pred==label,1], color=c, marker='.', \n",
    "                   s=50, alpha=0.75)\n",
    "\n",
    "#for idx in cluster_method.core_sample_indices_:\n",
    "#    ax.scatter(X[idx, 0], X[idx, 1], c='k', marker='d', s=100, alpha=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "- Requires that the user specifies the number of clusters (n_clusters)\n",
    "- Linkage criterion measures proximity between clusters\n",
    "    - average linkage is the average distance between one element of a cluster and all the others\n",
    "    - The user can define the distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = AgglomerativeClustering(n_clusters=2, linkage=\"average\", affinity=\"cityblock\")\n",
    "pred =cluster_method.fit_predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c=pred[Y==0], marker='x', \n",
    "           s=50, alpha=0.75, label='class 1', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.n_clusters)\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c=pred[Y==1], marker='o', \n",
    "           s=50, alpha=0.75, label='class 2', cmap=plt.cm.Accent, vmin=0, vmax=cluster_method.n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift\n",
    "- Based on kernel density estimation (KDE)\n",
    "- Does not need to specify the number of clusters\n",
    "- It finds the modes of the data distribution by differentiating the KDE\n",
    "- The user needs to provide the bandwidth for the Gaussian kernels in the KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = MeanShift(bandwidth=1.0, min_bin_freq=5)\n",
    "pred =cluster_method.fit_predict(X)\n",
    "\n",
    "n_clusters = len(np.unique(pred)) \n",
    "print(\"Number of clusters: %d\" %(n_clusters))\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for label, c in zip(np.unique(pred), plt.cm.Accent(np.linspace(0, 1, n_clusters))):\n",
    "    if label == -1: # noise cluster\n",
    "        ax.scatter(X[pred==label, 0], X[pred==label,1], color=c, marker='x', \n",
    "                   s=50, alpha=0.75)\n",
    "    else:\n",
    "        ax.scatter(X[pred==label, 0], X[pred==label,1], color=c, marker='.', \n",
    "                   s=50, alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture models\n",
    "\n",
    "- Clusters are modelled as full-covariance Gaussians\n",
    "- Outputs are probabilistic\n",
    "- The algorithms learns the mixing ratios and the Gaussian parameters\n",
    "- Uses Expectation-Maximization\n",
    "- There is a variational implementation in sklearn.mixture.BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_method = GaussianMixture(n_components=2).fit(X)\n",
    "pred = cluster_method.predict_proba(X)[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', \n",
    "           s=50, alpha=0.75, label='class 1')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', \n",
    "           s=50, alpha=0.75, label='class 2')\n",
    "Z = cluster_method.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "cr = ax.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu, alpha=0.5)\n",
    "plt.colorbar(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the number of clusters using K-means objective\n",
    "- The objective function of K-means is the sum of squared errors\n",
    "- The score function gives the negative of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # number of samples\n",
    "X, Y = make_blobs(n_samples=N, centers=[[1, 1], [-1, -1], [3, -1]], cluster_std=0.6)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', \n",
    "           s=50, alpha=0.75, label='class 1')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', \n",
    "           s=50, alpha=0.75, label='class 2')\n",
    "ax.scatter(X[Y==2, 0], X[Y==2, 1], c='k', marker='+', \n",
    "           s=50, alpha=0.75, label='class 3')\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []\n",
    "for k in range(10):\n",
    "    cluster_method = KMeans(n_clusters=k+1, init='random', n_init=10).fit(X)\n",
    "    obj.append(-cluster_method.score(X))\n",
    "plt.plot(np.linspace(1, 10, num=len(obj)), obj, 'o-')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Kmeans objective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
